<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[lepus数据库监控系统快速使用]]></title>
    <url>%2F2018%2F12%2F20%2Flepus%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1 介绍 Lepus(天兔)数据库企业监控系统是一套由专业DBA针对互联网企业开发的一款专业、强大的企业数据库监控管理系统，企业通过Lepus可以对数据库的实时健康和各种性能指标进行全方位的监控。目前已经支持MySQL、Oracle、MongoDB、Redis数据库的全面监控. Lepus可以在数据库出现故障或者潜在性能问题时,根据用户设置及时将数据库的异常进行报警通知到数据库管理员进行处理和优化,帮助企业解决数据库性能监控问题，及时发现性能和瓶颈,避免由数据库潜在问题造成的直接经济损失。Lepus能够查看各种实时性能状态指标，并且对监控、性能数据进行统计分析，从运维者到决策者多个层面的视角，查看相关报表。帮助决策者对未来数据库容量进行更好的规划，从而降低了硬件成本。 2 环境部署说明 官方说明文档中是指导安装lamp环境配置监控系统。通过搜索发现已制作docker镜像，因此我们直接使用docker镜像快速搭建环境。操作系统为centos7.4,ip地址为：192.168.2.75 3 部署步骤 注：安装docker前请关闭selinux，和配置防火墙能够开放端口(80和3306端口) 3.1 安装docker1yum install docker -y 3.2 启动docker服务，并设置开机自启动1systemctl start docker &amp;&amp; systemctl enable docker 3.3 创建数据库存放文件夹1mkdir -p /opt/mysql 3.4 启动镜像123docker run -d --name=lepus -p 80:80 -p 3306:3306 \-v /opt/mysql:/var/lib/mysql -v /etc/localtime:/etc/localtime:ro \georce/lepus 4 使用系统4.1 登陆系统123url:http://192.168.2.75/用户名：admin密码：Lepusadmin 4.2 登陆系统后界面示例截图展示 4.3 详细使用请参照官方文档 官方文档连接 4.4 参考地址 https://github.com/Georce/lepus]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql配置文件示例]]></title>
    <url>%2F2018%2F12%2F05%2Fmysql%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[介绍 本文通过举例演示生产环境中mysql配置文件如何进行配置，用于参考。 配置文件名称my.cnf 文件目录:一般为/etc/my.cnf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# MySQL configuration is created by yanghang[client]port = 3306socket = /tmp/mysql.sockuser=rootpassword=123456#The MySQL server###############Basic###############[mysqld]server-id=71report-host=192.168.2.71port = 3306user = mysql#禁止域名解析skip-name-resolveskip-external-locking#数据目录basedir=/data/mysqldatadir=/data/mysql/datasocket=/tmp/mysql.sock#服务端默认编码character_set_server = utf8#服务端默认的对比规则，排序规则collation_server=utf8_general_ci#slave库需要设置#read_only=1#超时时间wait_timeout=100interactive_timeout=100#事件开关event_scheduler = OFF###############binlog################不强制限制存储函数创建,这个变量也适用于触发器创建log_bin_trust_function_creators = 1#binlog存储地址log_bin = /data/mysql/logs/binlog/mysql-bin#binlog格式binlog_format = row#binlog过期时间expire-logs-days = 7#binlog缓存日志binlog_cache_size = 2M#控制数据库的binlog是否刷新到磁盘，高并发的环境开启对IO影响很大#sync_binlog = 1###主主复制时使用###修改将主从的offset分别设成1和2#自增初始值#auto_increment_offset = 1#自增间隔#auto_increment_increment = 2###############log_error################错误日志配置log_error = /data/mysql/logs/mysql-error.log###############slow log###############slow_query_log = 1slow_query_log_file = /data/mysql/logs/mysql-slow.loglong_query_time = 5###############relay日志#############relay_log = /data/mysql/logs/relay/mysql-relay-bin.logrelay_log_info_file = /data/mysql/logs/relay/mysql-relay-log.inforelay_log_purge=0###############半同步复制##############plugin-load = &quot;rpl_semi_sync_master=semisync_master.so;rpl_semi_sync_slave=semisync_slave.so&quot;#rpl-semi-sync-master-enabled = 1#rpl-semi-sync-slave-enabled = 1#rpl_semi_sync_master_wait_no_slave = 1#rpl_semi_sync_master_timeout = 1000###############innodb##############innodb_data_file_path = ibdata1:2G:autoextendinnodb_file_per_table = 1innodb_open_files = 500innodb_write_io_threads = 32innodb_read_io_threads = 32#一般设置为内存的80%innodb_buffer_pool_size = 1Ginnodb_file_per_table = 1#生产环境可以设置为1024Minnodb_log_file_size = 50Minnodb_log_buffer_size = 64Minnodb_log_files_in_group = 3innodb_file_format = Barracudainnodb_flush_log_at_trx_commit = 2innodb_buffer_pool_instances = 8#5.6.42报已InnoDB: Warning: Using innodb_additional_mem_pool_size is DEPRECATED. This option may be removed in future releases, together with the option innodb_use_sys_malloc and with the InnoDB&apos;s internal memory allocator.#innodb_additional_mem_pool_size = 16Minnodb_lock_wait_timeout = 10innodb_flush_method = O_DIRECTinnodb_change_buffering = allinnodb_purge_threads = 1innodb_purge_batch_size = 32transaction_isolation = READ-COMMITTED###############per_thread_buffers#############max_connections = 1024max_user_connections = 1000max_connect_errors = 10000lower_case_table_names = 1key_buffer_size = 64Mtable_open_cache = 6144table_definition_cache = 4096sort_buffer_size = 512Kread_buffer_size = 512Kjoin_buffer_size = 512Ktmp_table_size = 64Mmax_heap_table_size = 64Mquery_cache_type=0query_cache_size=0bulk_insert_buffer_size = 32Mthread_cache_size = 64thread_stack = 256Kmax_allowed_packet = 1024M#mysql5.6和mysql5.7默认的sql_mode不一样#sql_mode = STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION[mysqld_safe]#若要开启此选项则需要安装jemalloc#malloc-lib=/usr/lib64/libjemalloc.so.1open_files_limit = 65535[mysqldump]quickmax_allowed_packet = 1024M net_buffer_length = 16384user=rootpassword=123456[mysql]auto-rehash[mysqlhotcopy]interactive-timeout]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xtrabackup备份使用指南]]></title>
    <url>%2F2018%2F12%2F04%2Fxtrabackup%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[介绍 本文用来介绍如何使用percona公司的xtrabackup备份工具 1参考资料：https://www.cnblogs.com/shenxm/p/7862247.html 安装1yum localinstall percona-xtrabackup-24-2.4.11-1.el7.x86_64.rpm -y 使用 由于innobackupex命令既可以备份innodb又可以备份myisam表，因此主要介绍此工具的用法 12# 备份slave库上的数据。加上此参数会输出master库上的binlog信息--slave-info 全备备份1234567891011121314151.全量备份/usr/bin/innobackupex --defaults-file=/etc/my.cnf --socket=/var/lib/mysql/mysql.sock --host=127.0.0.1 --user=root --password=mvtech123 /data/20181129 --no-timestamp --slave-info --parallel=2 --throttle=8002.恢复全量备份 2.1 关闭mysql 2.2 重命名mysql的数据目录和日志目录 mv /data/mysql/data /data/mysql/data_20181129 mv /data/mysql/log /data/mysql/log_20181129 2.3 应用日志 innobackupex --apply-log --use-memory=200G /data/20181129/ 2.4 恢复 注：恢复有两种方式--move-back 和--copy-back，move-back速度快，但备份文件被move后就不存在，请根据实际情况进行选择 innobackupex --defaults-file=/etc/my.cnf --move-back /data/20181129/ 2.5 修改权限 chown -R mysql:mysql /data/mysql 2.6 启动mysql service mysqld start 增量备份 增量备份是在全量备份的基础上进行 1234567891011121314151617181920211. 增量备份 innobackupex --defaults-file=/etc/my.cnf --host=127.0.0.1 --user=root --password=mvtech123 --parallel=4 --throttle=400 --incremental-basedir=/data/20181129 --no-timestamp --slave-info --incremental /data/201811302. 恢复 2.1 关闭mysql 2.2 重命名mysql的数据目录和日志目录 mv /data/mysql/data /data/mysql/data_20181129 mv /data/mysql/log /data/mysql/log_20181129 2.3 对全量备份做prepare innobackupex --apply-log --redo-only /data/20181129 2.4 对增量备份做prepare --redo-only：若只有一个增量备份或是最后那个增量备份文件，那么不需要这个选项，原因同上。也就是说这个选项不能用于最后一个增量备份进行prepare。 --incremental-dir=：此选项对应的目录为增量备份文件的目录 innobackupex --apply-log [--redo-only] /data/20181129 --incremental-dir=/data/20181130 2.5 查看prepare情况 查看全量备份文件中的xtrabackup_checkpoints 发现last_lsn = 508150192已经和最后一次备份一致 2.6 恢复 注：恢复有两种方式--move-back 和--copy-back，move-back速度快，但备份文件被move后就不存在，请根据实际情况进行选择 innobackupex --defaults-file=/etc/my.cnf --move-back /data/20181129/ 2.7 修改权限 chown -R mysql:mysql /data/mysql 2.8 启动mysql service mysqld start 附录1 生产全备示例脚本1234567891011121314151617181920#!/bin/bash#This script is xtrabackup full backup mysql#The Author is mvtech by 2018#格式化日期today=$(date +%Y%m%d)deleteDay=`date -d &quot;-1 days&quot; +%Y%m%d`echo &quot;--------------------------------------------------------&quot;echo &quot;-------------------Today is $today----------------------&quot;echo &quot;--------------------Start backup------------------------&quot;echo &quot;--------------------------------------------------------&quot;#定义变量blxxDir=&quot;/data/xtrabackup/$today&quot;delDir=&quot;/data/xtrabackup/$deleteDay&quot;#创建备份文件夹mkdir -p $blxxDir#全量备份/usr/bin/innobackupex --defaults-file=/etc/my.cnf --socket=/data/mysql/mysql.sock --host=10.99.0.81 --user=weihu --password=Mvtech123!@ $blxxDir --slave-info --no-timestamp --parallel=8 --throttle=800 #删除2天以前的备份rm -rf $delDir 附录2 备份文件夹里文件说明 序号 文件名称 文件用途 1 xtrabackup_binlog_info 记录导出mysql的binlog信息 2 xtrabackup_checkpoints 记录备份方式 3 xtrabackup_info 记录此次备份的详细信息 4 xtrabackup_logfile 备份的重做日志文件 5 xtrabackup_slave_info 记录备份slave节点时主机上的binlog信息，需要添加–slave-info参数]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql常用统计sql]]></title>
    <url>%2F2018%2F12%2F02%2Fmysql%E5%B8%B8%E7%94%A8%E7%BB%9F%E8%AE%A1sql%2F</url>
    <content type="text"><![CDATA[介绍 系统维护中经常会用到统计mysql数据库中数据量大小的需要，因此今天整理了一下常用的几个mysql统计数据库信息的sql，方便以后使用。mysql5.6中自带的information_schema数据库，此库中装的是mysql的元数据，包括数据库信息、数据库中表的信息等。所以要想查询数据库占用磁盘的空间大小可以通过对information_schema数据库进行操作。 information_schema中的表主要有： schemata表：这个表里面主要是存储在mysql中的所有的数据库的信息 tables表：这个表里存储了所有数据库中的表的信息，包括每个表有多少个列等信息。 columns表：这个表存储了所有表中的表字段信息。 statistics表：存储了表中索引的信息。 user_privileges表：存储了用户的权限信息。 schema_privileges表：存储了数据库权限。 table_privileges表：存储了表的权限。 column_privileges表：存储了列的权限信息。 character_sets表：存储了mysql可以用的字符集的信息。 collations表：提供各个字符集的对照信息。 collation_character_set_applicability表：相当于collations表和character_sets表的前两个字段的一个对比，记录了字符集之间的对照信息。 table_constraints表：这个表主要是用于记录表的描述存在约束的表和约束类型。 key_column_usage表：记录具有约束的列。 routines表：记录了存储过程和函数的信息，不包含自定义的过程或函数信息。 views表：记录了视图信息，需要有show view权限。 triggers表：存储了触发器的信息，需要有super权限。 常用sql统计mysql中每个库各自占用空间大小，以MB为单位1SELECT TABLE_SCHEMA as '数据库名称' , concat(SUM(data_length) / 1024 / 1024, ' MB') AS '数据长度( MB)' , concat(SUM(index_length) / 1024 / 1024, 'MB') AS '索引长度( MB)',CONCAT((SUM(data_length)+SUM(index_length))/1024/1024, ' MB') as '数据库大小( MB)' FROM information_schema.tables GROUP BY TABLE_SCHEMA ORDER BY SUM(data_length)+SUM(index_length) DESC; 统计mysql指定库中所有表所占用的DATA_LENGTH和INDEX_LENGTH大小，以MB为单位，并按表占用空间进行排序1SELECT TABLE_NAME as '表名称' , CONCAT(DATA_LENGTH/1024/1024, ' MB') AS '数据长度( MB)' , CONCAT(INDEX_LENGTH/1024/1024, ' MB') AS '索引长度( MB)',CONCAT((DATA_LENGTH+INDEX_LENGTH)/1024/1024, ' MB') as '表占用空间大小( MB)' FROM information_schema.tables WHERE TABLE_SCHEMA = 'blxx_netmanager' GROUP BY TABLE_NAME ORDER BY DATA_LENGTH+INDEX_LENGTH DESC; 查询指定库中表占用空间大小1SELECT TABLE_SCHEMA AS '数据库名称', TABLE_NAME AS '表名' , concat(SUM(data_length) / 1024 / 1024, ' MB') AS '数据长度( MB)' , concat(SUM(index_length) / 1024 / 1024, 'MB') AS '索引长度( MB)',CONCAT((SUM(data_length)+SUM(index_length))/1024/1024, ' MB') as '数据库大小( MB)' FROM information_schema.tables WHERE table_schema = '库名' AND TABLE_NAME = '表名'; 数据库占用总空间1select concat(sum(DATA_LENGTH/1024/1024)+SUM(index_length/1024/1024),' MB') as '数据库占用总空间( MB)' from information_schema.tables;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cdh5.11.2快速配置文档]]></title>
    <url>%2F2018%2F12%2F01%2FCDH5.11.2%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[1 概述 本文档用于搭建impala开发测试环境,配置yarn-ha和hdfs-ha 2 服务器规划配置 具体请参考excel文档的部署规划sheet 3 部署软件版本和下载地址123456789101112mysql5.7.24:cdh相关软件下载地址：http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.11.2_x86_64.tar.gzhttp://archive.cloudera.com/cdh5/parcels/5.11.2.4/CDH-5.11.2-1.cdh5.11.2.p0.4-el7.parcelhttp://archive.cloudera.com/cdh5/parcels/5.11.2.4/CDH-5.11.2-1.cdh5.11.2.p0.4-el7.parcel.sha1 注：修改文件名称把sha1中的1去掉http://archive.cloudera.com/cdh5/parcels/5.11.2.4/manifest.jsonjdk-8u144-linux-x64nginx1.14ansiblesshpass用法：sshpass -p &apos;1w98.77B32&apos; ssh -o StrictHostKeyChecking=no root@168.1.2.180 &quot;hostnamectl set-hostname gxjh01&quot; 4 配置4.1 安装配置管理工具ansible 此工具用于批量配置管理服务器 4.1.1 安装1yum install ansible -y 4.1.2 修改配置文件 修改/etc/ansible/ansible.cfg，解掉注释1host_key_checking = False 4.1.3 配置ansible组 编辑/etc/ansible/hosts 12345[cdh]168.1.2.180 ansible_ssh_user=root ansible_ssh_pass=1w98.77B32168.1.2.181 ansible_ssh_user=root ansible_ssh_pass=1w98.77B32168.1.2.182 ansible_ssh_user=root ansible_ssh_pass=1w98.77B32168.1.2.183 ansible_ssh_user=root ansible_ssh_pass=1w98.77B32 4.2 初始化服务器环境4.2.1 执行脚本 关闭selinux，配置防火墙，配置ntp，添加sudo用户weihu，修改文件描述符，配置主机/etc/hosts文件。 在ansible中执行1ansible cdh -m script -a &apos;/root/centos7_init.sh&apos; -f 5 cento7_init.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#!/bin/bash# Filename: centos7_init.sh# Revision: 1.0# Date: 2016/12/15# Author: YangHang# Email: 13716320887@139.com# Website: no# Description: centos7系统初始化#1.定义配置yum源函数--(注：若程序不能联网，则需要配置本地yum源，将地址指向此处)function yum()&#123;touch /etc/yum.repos.d/mysql5.7.repocat &gt; /etc/yum.repos.d/mysql5.7.repo &lt;&lt; EOF[mysql5.7]name=mysql5.7baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql57-community-el7/enabled=1gpgcheck=0EOF&#125;#2.定义配置NTP函数function ntp()&#123;/usr/bin/yum -y install ntp#修改配置文件echo &apos;&apos; &gt;/etc/ntp.confcat &gt;&gt;/etc/ntp.conf &lt;&lt; EOFdriftfile /var/lib/ntp/driftrestrict default nomodify notrap nopeer noqueryrestrict 127.0.0.1 restrict ::1server ntp1.aliyun.comserver ntp2.aliyun.comserver ntp3.aliyun.comserver ntp4.aliyun.comserver ntp5.aliyun.comincludefile /etc/ntp/crypto/pwkeys /etc/ntp/keysdisable monitorEOF#同步hwclockcat &gt;&gt;/etc/sysconfig/ntpd&lt;&lt;EOF#Command line options for ntpdSYNC_HWCLOCK=yesOPTIONS=&quot;-g&quot;EOF#使用ntpdate命令校验时间/usr/sbin/ntpdate ntp2.aliyun.com &amp;&amp; /usr/sbin/hwclock -w/usr/bin/systemctl stop chronyd &amp;&gt; /dev/null/usr/bin/systemctl disable chronyd &amp;&gt; /dev/null/usr/bin/systemctl start ntpd &amp;&gt; /dev/null/usr/bin/systemctl enable ntpd &amp;&gt; /dev/null&#125;#3.定义关闭防火墙函数 如果机房漏扫建议将防火墙开启，此函数不执行function close_firewalld()&#123; /usr/bin/systemctl stop firewalld.service &amp;&gt; /dev/null /usr/bin/systemctl disable firewalld.service &amp;&gt; /dev/null&#125;#4.定义关闭selinux函数function close_selinux()&#123; setenforce 0 sed -i &apos;s#SELINUX=enforcing#SELINUX=disabled#g&apos; /etc/selinux/config&#125;#5.调整文件描述符function optimization()&#123;echo &quot;* soft nofile 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard nofile 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* soft nproc 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard nproc 65536&quot; &gt;&gt; /etc/security/limits.conf&#125;#6.配置hostsfunction host()&#123;cat &gt;&gt; /etc/hosts &lt;&lt; EOF168.1.2.180 gxjh01168.1.2.181 gxjh02168.1.2.182 gxjh03168.1.2.183 gxjh04EOF&#125;#7.添加sudo-weihu用户function add_user()&#123;useradd weihuecho &quot;weihu@123!&quot;|passwd --stdin weihuhistory -csed -i &apos;91a weihu ALL=(ALL) NOPASSWD:ALL&apos; /etc/sudoers&#125;#8.配置jdk环境变量 注：此处只是function jdk()&#123;cat &gt;&gt; /etc/profile &lt;&lt; EOFexport JAVA_HOME=/usr/local/java/jdk1.8.0_144export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$JAVA_HOME/bin:$PATHEOF&#125;#9.优化cdh相关参数function cdh()&#123;echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho &quot;vm.swappiness = 10&quot; &gt;&gt; /etc/sysctl.conf/usr/sbin/sysctl -p&#125;#10.配置rc.local 开机自启动function rc()&#123;chmod +x /etc/rc.d/rc.localcat &gt;&gt; /etc/rc.d/rc.local &lt;&lt; EOFecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled#/opt/cm-5.11.2/etc/init.d/cloudera-scm-server restart/opt/cm-5.11.2/etc/init.d/cloudera-scm-agent restartEOF&#125;#11.服务器用户登录限制 禁止用户登录，禁止使用dns(有的时候会出现ssh和ftp登录时间很长)function ssh()&#123;sed -i &apos;s/#PermitRootLogin/PermitRootLogin/g&apos; /etc/ssh/sshd_configsed -i &quot;s/#UseDNS no/UseDNS no/g&quot; /etc/ssh/sshd_configsystemctl restart sshd&#125;#初始化方法function init()&#123; yum; ntp; close_firewalld; close_selinux; optimization; host; add_user; jdk; cdh; rc;&#125;init 4.2.2 配置主机互信12345678-- 各服务器执行ssh-keygenansible cdh -m shell -a &quot;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa&quot; -f 10-- 生成authorized_keys文件ansible cdh -m shell -a &quot;cat /root/.ssh/id_rsa.pub&quot; -f 10 &gt; /tmp/authorized_keys-- 去除多余的行sed -i &apos;/SUCCESS/d&apos; /tmp/authorized_keys-- 分发authorized_keys至每台服务器ansible cdh -m copy -a &quot;src=/tmp/authorized_keys dest=/root/.ssh/ owner=root group=root mode=0644&quot; -f 10 4.2.3 配置jdk 由于4.2.1 中脚本已包含配置环境变量，因此直接执行以下操作即可12345ansible cdh -m copy -a &quot;src=/home/data/software/jdk-8u144-linux-x64.tar.gz dest=/root/jdk-8u144-linux-x64.tar.gz owner=root group=root mode=0644&quot; -f 10ansible cdh -m shell -a &quot;mkdir -p /usr/local/java/ &quot; -f 5ansible cdh -m shell -a &quot;tar -zxf /root/jdk-8u144-linux-x64.tar.gz -C /usr/local/java/ &quot; -f 5ansible cdh -m shell -a &quot;source /etc/profile&amp;&amp; java -version &quot; -f 5 4.3 配置mysql mysql安装在168.1.2.180 服务器中 4.3.1 配置mysql5.7.repo 文件路径:/etc/yum.repos.d/12345[mysql5.7]name=mysql5.7baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql57-community-el7/enabled=1gpgcheck=0 4.3.2 安装 注：这些服务器安装了多余的包需先卸载掉：yum -y remove mariadb-libs1234yum -y install mysql-community-servermkdir -p /home/data/mysql/datamkdir -p /home/data/mysql/logschown -R mysql:mysql /home/data/mysql 4.3.3 mysql配置文件 /etc/my.cnf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# mysql配置文件[mysqld]socket = /var/lib/mysql/mysql.sock#禁止域名解析skip-name-resolve#数据目录datadir=/home/data/mysql/data#mysql的server-idserver-id=249#超时时间wait_timeout=100interactive_timeout=100### binlog设置 #### binlog日志设置# 不强制限制存储函数创建,这个变量也适用于触发器创建log_bin_trust_function_creators = 1# binlog存储地址log_bin = /home/data/mysql/logs/mysql-bin# binlog格式binlog_format = row# binlog过期时间expire-logs-days = 7# binlog缓存日志binlog_cache_size = 2M# 自增初始值auto_increment_offset = 1# 自增间隔auto_increment_increment = 2 # 错误日志配置log_error = /home/data/mysql/logs/mysql-error.log# 慢日志配置slow_query_log = 1slow_query_log_file = /home/data/mysql/logs/mysql-slow.loglong_query_time = 5### innodb ###innodb_write_io_threads = 32innodb_read_io_threads = 32innodb_buffer_pool_size = 1Ginnodb_file_per_table = 1innodb_log_file_size = 50Minnodb_log_buffer_size = 64M### 优化配置 ###max_connections = 1024max_connect_errors = 1000lower_case_table_names = 1key_buffer_size = 64Mtable_open_cache = 6144table_definition_cache = 4096sort_buffer_size = 512Kread_buffer_size = 512Kjoin_buffer_size = 512Ktmp_table_size = 64Mmax_heap_table_size = 64M# 接收的数据包大小max_allowed_packet = 1024M#mysql5.6和mysql5.7默认的sql_mode不一样#sql_mode = STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION# 开启查询缓存explicit_defaults_for_timestamp=true 4.3.4 初始化mysql库 初始化mysql库1mysqld --initialize-insecure --user=mysql 4.3.5 启动mysql12systemctl start mysqldsystemctl enable mysqld 4.3.6 执行mysql_secure_installation1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@localhost ~]# mysql_secure_installationSecuring the MySQL server deployment.Connecting to MySQL using a blank password.VALIDATE PASSWORD PLUGIN can be used to test passwordsand improve security. It checks the strength of passwordand allows the users to set only those passwords which aresecure enough. Would you like to setup VALIDATE PASSWORD plugin?Press y|Y for Yes, any other key for No: nPlease set the password for root here.New password: Re-enter new password: By default, a MySQL installation has an anonymous user,allowing anyone to log into MySQL without having to havea user account created for them. This is intended only fortesting, and to make the installation go a bit smoother.You should remove them before moving into a productionenvironment.Remove anonymous users? (Press y|Y for Yes, any other key for No) : YSuccess.Normally, root should only be allowed to connect from&apos;localhost&apos;. This ensures that someone cannot guess atthe root password from the network.Disallow root login remotely? (Press y|Y for Yes, any other key for No) : ySuccess.By default, MySQL comes with a database named &apos;test&apos; thatanyone can access. This is also intended only for testing,and should be removed before moving into a productionenvironment.Remove test database and access to it? (Press y|Y for Yes, any other key for No) : y - Dropping test database...Success. - Removing privileges on test database...Success.Reloading the privilege tables will ensure that all changesmade so far will take effect immediately.Reload privilege tables now? (Press y|Y for Yes, any other key for No) : ySuccess.All done! [root@localhost ~]# 4.3.7 创建hive库1CREATE DATABASE `hive` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 4.4 配置CDH CDH包含server和agent两个，server用于管理，agent用于承载各种角色服务，本次规划使用在gxjh01上配置server+agent，另外三台运行agent 4.4.1 配置cm-server 在gxjh01服务器上执行，由于服务器中存储空间大不放在/home/目录中，考虑到cdh默认在/opt目录下因此，使用软链接将/opt/cloudera 和/opt/cm-11.2目录链接到/home/data/cloudera 和/home/data/cm-11.2下12345678910111213141516171819202122232425#初始化安装文件tar zxvf cloudera-manager-centos7-cm5.11.2_x86_64.tar.gz -C /home/data/mkdir -p /opt/cloudera/parcel-repo/#添加软连接ln -s /home/data/cloudera /opt/clouderaln -s /home/data/cm-5.11.2 /opt/cm-5.11.2chmod 777 /home/data/software/mysql-connector-java-5.1.39-bin.jarcp /home/data/software/mysql-connector-java-5.1.39-bin.jar /opt/cm-5.11.2/share/cmf/lib/mysql-connector-java.jarmv CDH-5.11.2-1.cdh5.11.2.p0.4-el7.parcel.sha1 CDH-5.11.2-1.cdh5.11.2.p0.4-el7.parcel.shamv CDH-5.11.2-1.cdh5.11.2.p0.4-el7.parcel* /opt/cloudera/parcel-repo/mv manifest.json /opt/cloudera/parcel-repo/#添加用户useradd --system --home=/opt/cm-5.11.2/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm#初始化cm库update user set Grant_priv=&apos;Y&apos; where user=&apos;weihu&apos; and Host=&apos;%&apos;;/opt/cm-5.11.2/share/cmf/schema/scm_prepare_database.sh mysql cm -hlocalhost -uweihu -pweihu123 --scm-host % scm scm scmmkdir -p /var/lib/cloudera-scm-serverchown cloudera-scm:cloudera-scm /var/lib/cloudera-scm-server#修改agent链接的配置文件/opt/cm-5.11.2/etc/cloudera-scm-agent/config.iniserver_host=gxjh01#启动：/opt/cm-5.11.2/etc/init.d/cloudera-scm-server restart 4.4.2 配置cm-agent配置 将ansible配置组中gxjh01服务器注释掉 12345678910111213### 配置用户### 拷贝安装文件ansible cdh -m copy -a &quot;src=/opt/cm-5.11.2.tar.gz dest=/root/ owner=root group=root mode=0644&quot; -f 4#创建文件夹ansible cdh -m shell -a &quot;mkdir -p /home/data&quot; -f 4ansible cdh -m shell -a &quot;mkdir -p /home/data/cloudera&quot; -f 4#解压文件ansible cdh -m shell -a &quot;tar -xf /root/cm-5.11.2.tar.gz -C /home/data&quot; -f 4#做软连接ansible cdh -m shell -a &quot;ln -s /home/data/cloudera /opt/cloudera&quot; -f 4ansible cdh -m shell -a &quot;ln -s /home/data/cm-5.11.2 /opt/cm-5.11.2&quot; -f 4#配置用户ansible cdh -m shell -a &apos;useradd --system --home=/opt/cm-5.11.2/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm&apos; -f 10 4.4.3 启动agent和开机自启动123456#启动：/opt/cm-5.11.2/etc/init.d/cloudera-scm-agent start#开机自启动：chmod +x /etc/rc.d/rc.local#添加如下：/opt/cm-5.11.2/etc/init.d/cloudera-scm-agent restart 4.4.4 登陆cm-server控制台，配置集群4.4.4.1 登陆控制台 http://168.1.2.180:7180 用户名：admin 密码：admin 4.4.4.2 登陆控制台 接受协议 4.4.4.3 选择所要安装的版本 选择免费版本 4.4.4.4 弹出下图 4.4.4.5 选择待安装的主机 根据实际需求选择相应主机 4.4.4.6 弹出下图点击继续 4.4.4.7 弹出下图点击继续 4.4.4.8 弹出下图点击继续 4.4.4.9 弹出下图点击继续 4.4.4.10 选择自定义角色安装 hdfs+yarn+zookeeper+hive+impala 4.4.4.11 为每个角色选择主机 原则上namenode和datanode不得在同一台，resourcemanager角色单独在一台机器上，由于此次只有四台服务器，因此每台机器都放了多个角色。为Hive Metastore Server角色服务器添加mysql-connector-java.jar 1cp /opt/cm-5.11.2/share/cmf/lib/mysql-connector-java.jar /opt/cloudera/parcels/CDH-5.11.2-1.cdh5.11.2.p0.4/lib/hive/lib/ 4.4.4.12 配置hive连接的mysql源 cp /opt/cm-5.11.2/share/cmf/lib/mysql-connector-java.jar /opt/cloudera/parcels/CDH-5.11.2-1.cdh5.11.2.p0.4/lib/hive/lib/ 4.4.4.13 弹出下图,根据实际需求更改相应目录，点击继续，安装完成后集群配置完毕 4.4.4.14 配置yarn ha 启用高可用，选择备用主机 4.4.4.15 启用hdfs ha 4.4.4.16 配置namenode server名称 4.4.4.17 选择备用hdfs主机 4.4.4.18 设置目录 4.4.4.19 点击继续 4.4.4.20 至此，hdfs高可用完毕 4.4.5 配置nginx负载均衡impalad4.4.5.1 安装123456789101112131415161718192021222324252627282930#安装依赖yum install pcre-devel zlib-devel openssl-devel -y#编译./configure --prefix=/home/data/nginx --with-http_ssl_module --with-http_flv_module --with-http_stub_status_module --with-http_gzip_static_module --with-stream#安装make &amp;&amp; make install#编辑配置文件 cat /home/data/nginx/conf/nginx.confworker_processes 8;worker_rlimit_nofile 65535;error_log logs/error.log;pid logs/nginx.pid;events &#123; use epoll; worker_connections 65535;&#125;stream &#123; server &#123; listen 31050; proxy_pass impala; &#125; upstream impala &#123; server 168.1.2.180:21050; server 168.1.2.181:21050; server 168.1.2.182:21050; server 168.1.2.183:21050; &#125;&#125; 4.4.5.2 启动&amp;&amp;停止12启动：/home/data/nginx/sbin/nginx停止：pkill -9 nginx 4.4.5.3 连接地址 168.1.2.180:31050 4.4.6 java程序连接集群的配置文件 配置文件包含两个:hdfs-site.xml 和core-site.xml12/etc/hadoop/conf.cloudera.hdfs/core-site.xml/etc/hadoop/conf.cloudera.hdfs/hdfs-site.xml 4.5 集群维护 本章用来介绍如何操作cdh集群的启停 4.5.1 启动4.5.1.1 启动mysql mysql安装地址为gxjh01上1systemctl start mysql 4.5.1.2 启动cm-server cm-server部署在gxjh01上1opt/cm-5.11.2/etc/init.d/cloudera-scm-server start 4.5.1.3 启动cm-agent cm-agent部署在gxjh[01:04]上1opt/cm-5.11.2/etc/init.d/cloudera-scm-agent start 4.5.1.4 启动nginx nginx安装地址为gxjh01上1/home/data/nginx/sbin/nginx 4.5.1.5 登陆cm-server web界面管理集群 通过界面启动集群 4.5.2 关闭4.5.2.1 登陆cm-server web界面管理集群 通过界面关闭集群 4.5.2.2 关闭nginx nginx安装地址为gxjh01上1pkill -9 nginx 4.5.2.3 关闭cm-agent cm-agent部署在gxjh[01:04]上1opt/cm-5.11.2/etc/init.d/cloudera-scm-agent stop 4.5.2.4关闭cm-server cm-server部署在gxjh01上1opt/cm-5.11.2/etc/init.d/cloudera-scm-server stop 4.5.2.5 关闭mysql mysql安装地址为gxjh01上1systemctl stop mysql]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql的版本升级]]></title>
    <url>%2F2018%2F12%2F01%2Fmysql%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[介绍 本文介绍针对于Unix/Linux下二进制mysql的版本升级， 背景介绍 目前生产上四台数据库服务器，采用MHA架构，所有节点的mysql为5.6.35，采用编译安装源代码方式进行安装。客户在绿盟漏扫过程中发现一堆高危漏洞，因此需要进行数据库升级。 升级方案介绍 由于目前是MHA架构，因此计划先将三台slave节点进行升级。然后通过mha的在线切换功能将master节点漂移至备用master节点，再升级最后一台服务器。版本选择：采用直接使用mysql官方编译好的二进制安装包：mysql-5.6.42-linux-glibc2.12-x86_64.tar.gz 步骤 升级备机时先检查备机的复制状态。1show slave status\G 停止mysql服务1service mysqld stop 删除软连接 做mha的时候配置了两条软连接：/usr/local/bin/mysql /usr/local/bin/mysqlbinlog12rm -rf /usr/local/bin/mysqlrm -rf /usr/local/bin/mysqlbinlog 解压安装包12tar -zxf mysql-5.6.42-linux-glibc2.12-x86_64.tar.gz -C /datarm -rf /data/mysql-5.6.42-linux-glibc2.12-x86_64/data 备份原数据库安装文件 原数据库目录为：/data/mysql1mv /data/mysql /data/mysql_backup 做软连接123ln -s /data/mysql-5.6.42-linux-glibc2.12-x86_64 /data/mysqlln -s /data/mysql/bin/mysql /usr/local/bin/mysqlln -s /data/mysql/bin/mysqlbinlog /usr/local/bin/mysqlbinlog 拷贝数据12mv /data/mysql_backup/data /data/mysql-5.6.42-linux-glibc2.12-x86_64/datamv /data/mysql_backup/log /data/mysql-5.6.42-linux-glibc2.12-x86_64/log 修改权限1chown -R mysql:mysql /data/mysql/ 启动1service mysqld start 执行升级命令 升级日志记录 重启mysql1service mysqld restart 查看主从复制状态至此，slave节点mysql升级完毕将剩余两台也按上述方式进行升级使用mha在线切换功能切换master节点1masterha_master_switch --conf=/etc/masterha/app1.conf --master_state=alive --new_master_host=blxx-db07 --new_master_port=3306 --orig_master_is_new_slave --running_updates_limit=10000 --interactive=0 将之前主节点的mysql进行升级。此次升级完成]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mha入门实战]]></title>
    <url>%2F2018%2F11%2F01%2FMHA%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[介绍原理介绍 MHA（Master High Availability）目前在MySQL高可用方面是一个相对成熟的解决方案，它由日本DeNA公司youshimaton（现就职于Facebook公司）开发，是一套优秀的作为MySQL高可用性环境下故障切换和主从提升的高可用软件。在MySQL故障切换过程中，MHA能做到在0~30秒之内自动完成数据库的故障切换操作，并且在进行故障切换的过程中，MHA能在最大程度上保证数据的一致性，以达到真正意义上的高可用。 mha集群架构图 工作原理1234561.从宕机崩溃的master保存二进制日志事件（binlog events）;2.识别含有最新更新的slave；3.应用差异的中继日志（relay log）到其他的slave；4.应用从master保存的二进制日志事件（binlog events）；5.提升一个slave为新的master；6.使其他的slave连接新的master进行复制； 软件构成 MHA软件由两部分组成，Manager工具包和Node工具包，具体的说明如下。 Manager工具包1234567masterha_check_ssh 检查MHA的SSH配置状况masterha_check_repl 检查MySQL复制状况masterha_manger 启动MHAmasterha_check_status 检测当前MHA运行状态masterha_master_monitor 检测master是否宕机masterha_master_switch 控制故障转移（自动或者手动）masterha_conf_host 添加或删除配置的server信息 Node工具包 这些工具通常由MHA Manager的脚本触发，无需人为操作1234save_binary_logs 保存和复制master的二进制日志apply_diff_relay_logs 识别差异的中继日志事件并将其差异的事件应用于其他的slavefilter_mysqlbinlog 去除不必要的ROLLBACK事件（MHA已不再使用这个工具）purge_relay_logs 清除中继日志（不会阻塞SQL线程） 参考地址12https://github.com/yoshinorim/mha4mysql-manager/wiki/Installationhttp://www.cnblogs.com/gomysql/p/3675429.html 部署规划 本次部署准备三台服务器，一台master，一台备用master，一台slave；vip的产生使用keepalived（之前使用过ifconfig添加vip，但有的时候能够ping通有的时候不通，因此换成keepalived） 部署规划 序号 服务器主机名 服务器IP mha角色 用途 需要安装软件 1 node71 192.168.2.71 mha-manager+mha-node slave+mha-manager mysql+mha4mysql-node+mha4mysql-manager 2 node72 192.168.2.72 mha-node 备用master节点 mysql+mha4mysql-node+keepalived 3 node73 192.168.2.73 mha-node master节点 mysql+mha4mysql-node +keepalived yum源配置 若服务器能联网请添加阿里云epel.repo镜像源和mysql5.6源，若不能联网，需将需要的包下载下来自制yum源；生产环境很多不能连接互联网，可在一台可联网的测试机上配置epel源，通过yum install –downloadonly –downloaddir=/root/mha/ perl-Parallel-ForkManager perl-DBD-MySQL perl-Config-Tiny perl-Log-Dispatch mysql-community-server，下载到本地，通过createrepo创建yum源 安装包和相关脚本获取 文件存放在百度网盘中，链接：https://pan.baidu.com/s/12jqvyr_fM9qZTdP3sAbDFg 提取码：t2jv 序号 文件名称 文件用途 1 app1.conf mha-manager配置文件 2 install_mysql5.6.sh 脚本自动安装初始化mysql5.6 3 master_ip_failover 故障切换masterip脚本 4 master_ip_online_change 在线切换masterip脚本 5 mha4mysql-manager-0.57-0.el7.noarch.rpm mha-manager rpm安装包 6 mha4mysql-node-0.57-0.el7.noarch.rpm mha-node rpm安装包 7 send_report 切换ip发送邮件脚本 8 keepalived.conf 产生vip的配置文件 9 purge_relay_log.sh 自动删除relay日志脚本 10 mha4mysql-node-0.57.tar.gz mha-node 源码安装包 11 mha4mysql-manager-0.57.tar.gz mha-manager 源码安装包 操作系统环境 本次测试环境为centos7.4最小化安装，服务器可以访问外网 部署集群安装mysql 使用脚本完成mysql的部署，脚本使用yum进行安装，并完成相应的初始化 12bash +x install_mysql5.6.sh# 脚本执行完毕后mysql用户：root 密码：mvtech123 配置主从复制 配置成一主多从架构 12345678910111213141516171819201 添加主从复制账户 grant replication slave on *.* to repl@'192.168.2.%' identified by 'slave';2 添加mha管理账号，每台都添加 grant all privileges on *.* to root@"%" identified by "mvtech123";() grant all privileges on *.* to masterha@"%" identified by "masterha";(注：本次配置文件里设置的是root账号)3 查看主库binlog日志 show binary logs4 从库配置主从 CHANGE MASTER TO MASTER_HOST='192.168.2.72', MASTER_USER='repl', MASTER_PASSWORD='slave', MASTER_LOG_FILE='mysql-bin.000004', MASTER_LOG_POS=533;5 从库启动slave start slave;6 查看从库主从复制状态 show slave status;7 slave执行：set global relay_log_purge=0;(此处我已添加到脚本里，不用每次都执行)set global read_only=1; 配置mha集群配置各个节点之间的主机互信 本次使用ansible快速配置，具体ansible的配置在此不做赘述。12345678-- 各服务器执行ssh-keygenansible mha -m shell -a "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa" -f 100-- 生成authorized_keys文件ansible mha -m shell -a "cat /root/.ssh/id_rsa.pub" -f 100 &gt; /tmp/authorized_keys-- 去除多余的行sed -i '/SUCCESS/d' /tmp/authorized_keys-- 分发authorized_keys至每台服务器ansible mha -m copy -a "src=/tmp/authorized_keys dest=/root/.ssh/ owner=root group=root mode=0644" -f 100 配置node节点 node节点是除了manager角色以外的节点，此次试验为,node72，node731234#安装依赖包yum install perl-DBD-MySQL#安装node rpm包rpm -ivh mha4mysql-node-0.57-0.el7.noarch.rpm master节点和备用master节点需要配置keepalived来配置vip1234567891011121314151617181920212223241 安装yum install keepalived -y2 编辑配置文件/etc/keepalived/keepalived.conf(注：由于两台不同时启动，因此配置文件可以设置成一样)! Configuration File for keepalivedglobal_defs &#123;router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123;state BACKUPinterface ens3virtual_router_id 30priority 90advert_int 1nopreemptauthentication &#123;auth_type PASSauth_pass 11111&#125;virtual_ipaddress &#123;192.168.2.74&#125;&#125; 配置manager节点安装12345678910#安装依赖包yum install perl-Parallel-ForkManager perl-DBD-MySQL perl-Config-Tiny perl-Log-Dispatch#安装manager和node rpm包rpm -ivh mha4mysql-node-0.57-0.el7.noarch.rpmrpm -ivh mha4mysql-manager-0.57-0.el7.noarch.rpm#创建文件夹#存放mha-manager配置文件文件夹mkdir -p /etc/masterha#创建日志文件夹mkdir -p /var/log/masterha/app1 修改配置文件123456789101112131415161718192021222324252627[server default]manager_log=/var/log/masterha/app1/manager.logmanager_workdir=/var/log/masterha/app1master_binlog_dir=/mvtech/mysql/logsmaster_ip_failover_script=/usr/local/bin/master_ip_failovermaster_ip_online_change_script=/usr/local/bin/master_ip_online_changeuser=rootpassword=mvtech123ping_interval=1remote_workdir=/tmprepl_password=slaverepl_user=replreport_script=/usr/local/bin/send_reportsecondary_check_script=/usr/bin/masterha_secondary_check -s node72 -s node71ssh_user=root[server1]hostname=node73[server2]candidate_master=1hostname=node72port=3306[server3]hostname=node71port=3306 创建相应脚本,脚本需要有可执行权限 /usr/local/bin/master_ip_failover123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#!/usr/bin/env perluse strict;use warnings FATAL =&gt; 'all';use Getopt::Long;my ( $command, $ssh_user, $orig_master_host, $orig_master_ip, $orig_master_port, $new_master_host, $new_master_ip, $new_master_port);my $vip = '192.168.2.74/24';my $key = '1';my $ssh_start_vip = "/bin/systemctl start keepalived";my $ssh_stop_vip = "/bin/systemctl stop keepalived";GetOptions( 'command=s' =&gt; \$command, 'ssh_user=s' =&gt; \$ssh_user, 'orig_master_host=s' =&gt; \$orig_master_host, 'orig_master_ip=s' =&gt; \$orig_master_ip, 'orig_master_port=i' =&gt; \$orig_master_port, 'new_master_host=s' =&gt; \$new_master_host, 'new_master_ip=s' =&gt; \$new_master_ip, 'new_master_port=i' =&gt; \$new_master_port,);exit &amp;main();sub main &#123; print "\n\nIN SCRIPT TEST====$ssh_stop_vip==$ssh_start_vip===\n\n"; if ( $command eq "stop" || $command eq "stopssh" ) &#123; my $exit_code = 1; eval &#123; print "Disabling the VIP on old master: $orig_master_host \n"; &amp;stop_vip(); $exit_code = 0; &#125;; if ($@) &#123; warn "Got Error: $@\n"; exit $exit_code; &#125; exit $exit_code; &#125; elsif ( $command eq "start" ) &#123; my $exit_code = 10; eval &#123; print "Enabling the VIP - $vip on the new master - $new_master_host \n"; &amp;start_vip(); $exit_code = 0; &#125;; if ($@) &#123; warn $@; exit $exit_code; &#125; exit $exit_code; &#125; elsif ( $command eq "status" ) &#123; print "Checking the Status of the script.. OK \n"; exit 0; &#125; else &#123; &amp;usage(); exit 1; &#125;&#125;sub start_vip() &#123; `ssh $ssh_user\@$new_master_host \" $ssh_start_vip \"`;&#125;sub stop_vip() &#123; return 0 unless ($ssh_user); `ssh $ssh_user\@$orig_master_host \" $ssh_stop_vip \"`;&#125;sub usage &#123; print "Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\n";&#125; /usr/local/bin/master_ip_online_change 在线切换master的ip123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297#!/usr/bin/env perl# Copyright (C) 2011 DeNA Co.,Ltd.## This program is free software; you can redistribute it and/or modify# it under the terms of the GNU General Public License as published by# the Free Software Foundation; either version 2 of the License, or# (at your option) any later version.## This program is distributed in the hope that it will be useful,# but WITHOUT ANY WARRANTY; without even the implied warranty of# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the# GNU General Public License for more details.## You should have received a copy of the GNU General Public License# along with this program; if not, write to the Free Software# Foundation, Inc.,# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA## Note: This is a sample script and is not complete. Modify the script based on your environment.use strict;use warnings FATAL =&gt; 'all';use Getopt::Long;use MHA::DBHelper;use MHA::NodeUtil;use Time::HiRes qw( sleep gettimeofday tv_interval );use Data::Dumper;my $_tstart;my $_running_interval = 0.1;my ( $command, $orig_master_host, $orig_master_ip, $orig_master_port, $orig_master_user, $new_master_host, $new_master_ip, $new_master_port, $new_master_user, );my $vip = '192.168.2.74/24'; # Virtual IP my $key = "1"; my $ssh_start_vip = "/bin/systemctl start keepalived";my $ssh_stop_vip = "/bin/systemctl stop keepalived";my $ssh_user = "root";my $new_master_password='mvtech123';my $orig_master_password='mvtech123';GetOptions( 'command=s' =&gt; \$command, #'ssh_user=s' =&gt; \$ssh_user, 'orig_master_host=s' =&gt; \$orig_master_host, 'orig_master_ip=s' =&gt; \$orig_master_ip, 'orig_master_port=i' =&gt; \$orig_master_port, 'orig_master_user=s' =&gt; \$orig_master_user, #'orig_master_password=s' =&gt; \$orig_master_password, 'new_master_host=s' =&gt; \$new_master_host, 'new_master_ip=s' =&gt; \$new_master_ip, 'new_master_port=i' =&gt; \$new_master_port, 'new_master_user=s' =&gt; \$new_master_user, #'new_master_password=s' =&gt; \$new_master_password,);exit &amp;main();sub current_time_us &#123; my ( $sec, $microsec ) = gettimeofday(); my $curdate = localtime($sec); return $curdate . " " . sprintf( "%06d", $microsec );&#125;sub sleep_until &#123; my $elapsed = tv_interval($_tstart); if ( $_running_interval &gt; $elapsed ) &#123; sleep( $_running_interval - $elapsed ); &#125;&#125;sub get_threads_util &#123; my $dbh = shift; my $my_connection_id = shift; my $running_time_threshold = shift; my $type = shift; $running_time_threshold = 0 unless ($running_time_threshold); $type = 0 unless ($type); my @threads; my $sth = $dbh-&gt;prepare("SHOW PROCESSLIST"); $sth-&gt;execute(); while ( my $ref = $sth-&gt;fetchrow_hashref() ) &#123; my $id = $ref-&gt;&#123;Id&#125;; my $user = $ref-&gt;&#123;User&#125;; my $host = $ref-&gt;&#123;Host&#125;; my $command = $ref-&gt;&#123;Command&#125;; my $state = $ref-&gt;&#123;State&#125;; my $query_time = $ref-&gt;&#123;Time&#125;; my $info = $ref-&gt;&#123;Info&#125;; $info =~ s/^\s*(.*?)\s*$/$1/ if defined($info); next if ( $my_connection_id == $id ); next if ( defined($query_time) &amp;&amp; $query_time &lt; $running_time_threshold ); next if ( defined($command) &amp;&amp; $command eq "Binlog Dump" ); next if ( defined($user) &amp;&amp; $user eq "system user" ); next if ( defined($command) &amp;&amp; $command eq "Sleep" &amp;&amp; defined($query_time) &amp;&amp; $query_time &gt;= 1 ); if ( $type &gt;= 1 ) &#123; next if ( defined($command) &amp;&amp; $command eq "Sleep" ); next if ( defined($command) &amp;&amp; $command eq "Connect" ); &#125; if ( $type &gt;= 2 ) &#123; next if ( defined($info) &amp;&amp; $info =~ m/^select/i ); next if ( defined($info) &amp;&amp; $info =~ m/^show/i ); &#125; push @threads, $ref; &#125; return @threads;&#125;sub main &#123; if ( $command eq "stop" ) &#123; ## Gracefully killing connections on the current master # 1. Set read_only= 1 on the new master # 2. DROP USER so that no app user can establish new connections # 3. Set read_only= 1 on the current master # 4. Kill current queries # * Any database access failure will result in script die. my $exit_code = 1; eval &#123; ## Setting read_only=1 on the new master (to avoid accident) my $new_master_handler = new MHA::DBHelper(); # args: hostname, port, user, password, raise_error(die_on_error)_or_not $new_master_handler-&gt;connect( $new_master_ip, $new_master_port, $new_master_user, $new_master_password, 1 ); print current_time_us() . " Set read_only on the new master.. "; $new_master_handler-&gt;enable_read_only(); if ( $new_master_handler-&gt;is_read_only() ) &#123; print "ok.\n"; &#125; else &#123; die "Failed!\n"; &#125; $new_master_handler-&gt;disconnect(); # Connecting to the orig master, die if any database error happens my $orig_master_handler = new MHA::DBHelper(); $orig_master_handler-&gt;connect( $orig_master_ip, $orig_master_port, $orig_master_user, $orig_master_password, 1 ); ## Drop application user so that nobody can connect. Disabling per-session binlog beforehand #$orig_master_handler-&gt;disable_log_bin_local(); #print current_time_us() . " Drpping app user on the orig master..\n"; #FIXME_xxx_drop_app_user($orig_master_handler); ## Waiting for N * 100 milliseconds so that current connections can exit my $time_until_read_only = 15; $_tstart = [gettimeofday]; my @threads = get_threads_util( $orig_master_handler-&gt;&#123;dbh&#125;, $orig_master_handler-&gt;&#123;connection_id&#125; ); while ( $time_until_read_only &gt; 0 &amp;&amp; $#threads &gt;= 0 ) &#123; if ( $time_until_read_only % 5 == 0 ) &#123; printf"%s Waiting all running %d threads are disconnected.. (max %d milliseconds)\n", current_time_us(), $#threads + 1, $time_until_read_only * 100; if ( $#threads &lt; 5 ) &#123; print Data::Dumper-&gt;new( [$_] )-&gt;Indent(0)-&gt;Terse(1)-&gt;Dump . "\n" foreach (@threads); &#125; &#125; sleep_until(); $_tstart = [gettimeofday]; $time_until_read_only--; @threads = get_threads_util( $orig_master_handler-&gt;&#123;dbh&#125;, $orig_master_handler-&gt;&#123;connection_id&#125; ); &#125; ## Setting read_only=1 on the current master so that nobody(except SUPER) can write print current_time_us() . " Set read_only=1 on the orig master.. "; $orig_master_handler-&gt;enable_read_only(); if ( $orig_master_handler-&gt;is_read_only() ) &#123; print "ok.\n"; &#125; else &#123; die "Failed!\n"; &#125; ## Waiting for M * 100 milliseconds so that current update queries can complete my $time_until_kill_threads = 5; @threads = get_threads_util( $orig_master_handler-&gt;&#123;dbh&#125;, $orig_master_handler-&gt;&#123;connection_id&#125; ); while ( $time_until_kill_threads &gt; 0 &amp;&amp; $#threads &gt;= 0 ) &#123; if ( $time_until_kill_threads % 5 == 0 ) &#123; printf"%s Waiting all running %d queries are disconnected.. (max %d milliseconds)\n", current_time_us(), $#threads + 1, $time_until_kill_threads * 100; if ( $#threads &lt; 5 ) &#123; print Data::Dumper-&gt;new( [$_] )-&gt;Indent(0)-&gt;Terse(1)-&gt;Dump . "\n" foreach (@threads); &#125; &#125; sleep_until(); $_tstart = [gettimeofday]; $time_until_kill_threads--; @threads = get_threads_util( $orig_master_handler-&gt;&#123;dbh&#125;, $orig_master_handler-&gt;&#123;connection_id&#125; ); &#125; print "Disabling the VIP on old master: $orig_master_host \n"; &amp;stop_vip(); ## Terminating all threads print current_time_us() . " Killing all application threads..\n"; $orig_master_handler-&gt;kill_threads(@threads) if ( $#threads &gt;= 0 ); print current_time_us() . " done.\n"; #$orig_master_handler-&gt;enable_log_bin_local(); $orig_master_handler-&gt;disconnect(); ## After finishing the script, MHA executes FLUSH TABLES WITH READ LOCK $exit_code = 0; &#125;; if ($@) &#123; warn "Got Error: $@\n"; exit $exit_code; &#125; exit $exit_code; &#125; elsif ( $command eq "start" ) &#123; ## Activating master ip on the new master # 1. Create app user with write privileges # 2. Moving backup script if needed # 3. Register new master's ip to the catalog database# We don't return error even though activating updatable accounts/ip failed so that we don't interrupt slaves' recovery.# If exit code is 0 or 10, MHA does not abort my $exit_code = 10; eval &#123; my $new_master_handler = new MHA::DBHelper(); # args: hostname, port, user, password, raise_error_or_not $new_master_handler-&gt;connect( $new_master_ip, $new_master_port, $new_master_user, $new_master_password, 1 ); ## Set read_only=0 on the new master #$new_master_handler-&gt;disable_log_bin_local(); print current_time_us() . " Set read_only=0 on the new master.\n"; $new_master_handler-&gt;disable_read_only(); ## Creating an app user on the new master #print current_time_us() . " Creating app user on the new master..\n"; #FIXME_xxx_create_app_user($new_master_handler); #$new_master_handler-&gt;enable_log_bin_local(); $new_master_handler-&gt;disconnect(); ## Update master ip on the catalog database, etc print "Enabling the VIP - $vip on the new master - $new_master_host \n"; &amp;start_vip(); $exit_code = 0; &#125;; if ($@) &#123; warn "Got Error: $@\n"; exit $exit_code; &#125; exit $exit_code; &#125; elsif ( $command eq "status" ) &#123; # do nothing exit 0; &#125; else &#123; &amp;usage(); exit 1; &#125;&#125;# A simple system call that enable the VIP on the new master sub start_vip() &#123; `ssh $ssh_user\@$new_master_host \" $ssh_start_vip \"`;&#125;# A simple system call that disable the VIP on the old_mastersub stop_vip() &#123; `ssh $ssh_user\@$orig_master_host \" $ssh_stop_vip \"`;&#125;sub usage &#123; print"Usage: master_ip_online_change --command=start|stop|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\n"; die;&#125; /usr/local/bin/send_report 切换时发送邮件 注：发送邮件需要这台服务器能够连接互联网1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#!/usr/bin/perl# Copyright (C) 2011 DeNA Co.,Ltd.## This program is free software; you can redistribute it and/or modify# it under the terms of the GNU General Public License as published by# the Free Software Foundation; either version 2 of the License, or# (at your option) any later version.## This program is distributed in the hope that it will be useful,# but WITHOUT ANY WARRANTY; without even the implied warranty of# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the# GNU General Public License for more details.## You should have received a copy of the GNU General Public License# along with this program; if not, write to the Free Software# Foundation, Inc.,# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA## Note: This is a sample script and is not complete. Modify the script based on your environment.use strict;use warnings FATAL =&gt; 'all';use Mail::Sender;use Getopt::Long;#new_master_host and new_slave_hosts are set only when recovering master succeededmy ( $dead_master_host, $new_master_host, $new_slave_hosts, $subject, $body );my $smtp='smtp.139.com';my $mail_from='########@139.com';my $mail_user='########@139.com';my $mail_pass='########;my $mail_to=['########@139.com','########@baidu.com.cn'];GetOptions( 'orig_master_host=s' =&gt; \$dead_master_host, 'new_master_host=s' =&gt; \$new_master_host, 'new_slave_hosts=s' =&gt; \$new_slave_hosts, 'subject=s' =&gt; \$subject, 'body=s' =&gt; \$body,);mailToContacts($smtp,$mail_from,$mail_user,$mail_pass,$mail_to,$subject,$body);sub mailToContacts &#123; my ( $smtp, $mail_from, $user, $passwd, $mail_to, $subject, $msg ) = @_; open my $DEBUG, "&gt; /tmp/monitormail.log" or die "Can't open the debug file:$!\n"; my $sender = new Mail::Sender &#123; ctype =&gt; 'text/plain; charset=utf-8', encoding =&gt; 'utf-8', smtp =&gt; $smtp, from =&gt; $mail_from, auth =&gt; 'LOGIN', TLS_allowed =&gt; '0', authid =&gt; $user, authpwd =&gt; $passwd, to =&gt; $mail_to, subject =&gt; $subject, debug =&gt; $DEBUG &#125;; $sender-&gt;MailMsg( &#123; msg =&gt; $msg, debug =&gt; $DEBUG &#125; ) or print $Mail::Sender::Error; return 1;&#125;# Do whatever you want hereexit 0; 每台slave设置定时清除relay日志脚本 MHA在发生切换的过程中，从库的恢复过程中依赖于relay log的相关信息，所以这里要将relay log的自动清除设置为OFF，采用手动清除relay log的方式。在默认情况下，从服务器上的中继日志会在SQL线程执行完毕后被自动删除。但是在MHA环境中，这些中继日志在恢复其他从服务器时可能会被用到，因此需要禁用中继日志的自动删除功能。定期清除中继日志需要考虑到复制延时的问题。在ext3的文件系统下，删除大的文件需要一定的时间，会导致严重的复制延时。为了避免复制延时，需要暂时为中继日志创建硬链接，因为在linux系统中通过硬链接删除大文件速度会很快。（在mysql数据库中，删除大表时，通常也采用建立硬链接的方式） 1234567891011121314151617#!/bin/bash#此脚本添加至定时任务，每天凌晨四点执行，定时任务示例如下：#0 4 * * * /bin/bash /usr/local/bin/purge_relay_log.shuser=rootpasswd=mvtech123port=3306log_dir='/var/log/masterha/'#relay日志的目录work_dir='/mvtech/mysql/logs/relay'purge='/usr/bin/purge_relay_logs'if [ ! -d $log_dir ]then mkdir $log_dir -pfi$purge --user=$user --password=$passwd --disable_relay_log_purge --port=$port --workdir=$work_dir &gt;&gt; $log_dir/purge_relay_logs.log 2&gt;&amp;1 验证集群配置情况 验证ssh互信：masterha_check_ssh –conf=/etc/masterha/app1.conf 验证复制状态：masterha_check_repl –conf=/etc/masterha/app1.conf 验证集群状态：masterha_check_status –conf=/etc/masterha/app1.conf 启动manager服务1nohup masterha_manager --conf=/etc/masterha/app1.conf --ignore_last_failover &lt; /dev/null &gt; /var/log/masterha/app1/manager.log 2&gt;&amp;1 &amp; 集群可用状态 模拟故障，验证集群的可用性失败自动切换 模拟master主机msyql挂掉，并恢复集群关闭master主机1systemctl stop mysqld 查看manage日志 tail -f /var/log/masterha/app1/manager.log 通过查看日志，会看到自动切换的过程，并发送邮件 检查备用master是否提升为主节点 检查vip是否漂在node72节点上1ip addr 查看目前node72上是否还属于node73的slave节点12show slave status\G# 经检查 已不是slave节点 检查独立slave主机的主从复制状态 通过检查复制状态，发现现在node01节点的master节点变为node721show slave status\G 恢复集群123456781.通过备份工具将node72上数据同步至node73节点上，并以node02作为master节点配置node03位slave节点；2.在node03节点上执行sql： set global relay_log_purge=0; set global read_only=1;3.修改node01上集群的配置文件/etc/masterha/app1.conf，将原来node03作为master的配置改为node024.使用集群验证脚本验证ssh互信、复制状态等是否ok5.启动manager继续监控集群6.至此，集群恢复完毕 在线切换master 在许多情况下， 需要将现有的主服务器迁移到另外一台服务器上。 比如主服务器硬件故障，RAID 控制卡需要重建，将主服务器移到性能更好的服务器上等等。维护主服务器引起性能下降， 导致停机时间至少无法写入数据。 另外， 阻塞或杀掉当前运行的会话会导致主主之间数据不一致的问题发生。 MHA 提供快速切换和优雅的阻塞写入，这个切换过程只需要 0.5-2s 的时间，这段时间内数据是无法写入的。在很多情况下，0.5-2s 的阻塞写入是可以接受的。因此切换主服务器不需要计划分配维护时间窗口。 123456789101.停止mha集群监控 masterha_stop --conf=/etc/masterha/app1.conf2.执行切换命令 masterha_master_switch --conf=/etc/masterha/app1.conf --master_state=alive --new_master_host=node72 --new_master_port=3306 --orig_master_is_new_slave --running_updates_limit=10000 --interactive=03.验证node72是否变成master节点#从72切到73masterha_master_switch --conf=/etc/masterha/app1.conf_node72 --master_state=alive --new_master_host=node73 --new_master_port=3306 --orig_master_is_new_slave --running_updates_limit=10000 --interactive=0#从73切到72masterha_master_switch --conf=/etc/masterha/app1.conf --master_state=alive --new_master_host=node72 --new_master_port=3306 --orig_master_is_new_slave --running_updates_limit=10000 --interactive=0 失败手工切换12345678masterha_master_switch --master_state=dead --conf=/etc/masterha/app1.conf --dead_master_host=node73 --dead_master_port=3306 --new_master_host=node72 --new_master_port=3306 --ignore_last_failover --interactive=0``` # 维护mha集群## 停止集群### 关闭mha-manager监控脚本``` bashmasterha_stop --conf=/etc/masterha/app1.conf 关闭master节点上的虚地址12345/sbin/ifconfig ens3:1 down``` ### 关闭master节点上的mysql``` bashsystemctl stop mysqld 关闭两台slave节点上的mysql 分别在两台slave上执行1systemctl stop mysqld 至此，mha集群关闭结束启动集群启动两台slave上的mysql，两台备库设置123systemct start mysqldset global relay_log_purge=0;set global read_only=1; 启动master上的mysql1systemct start mysqld 启动master上的vip12/sbin/ifconfig ens3:1 192.168.2.74/24注：若使用keepalived则使用systemctl start keepalived 使用mha脚本监测集群状态：123masterha_check_ssh --conf=/etc/masterha/app1.conf masterha_check_repl --conf=/etc/masterha/app1.confmasterha_check_status --conf=/etc/masterha/app1.conf 若没有问题，则启动manager进行监控集群1nohup masterha_manager --conf=/etc/masterha/app1.conf --remove_dead_master_conf --ignore_last_failover &lt; /dev/null &gt; /var/log/masterha/app1/manager.log 2&gt;&amp;1 &amp; 检查集群状态1masterha_check_status --conf=/etc/masterha/app1.conf 至此集群启动完毕]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>高可用架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker入门指南]]></title>
    <url>%2F2018%2F10%2F01%2Fdocker%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[概述什么是dockerdocker优点 不需要安装操作系统，秒级启动，可充分利用服务器资源 方便部署 公司准备使用场景搭建演示环境 将程序统一打包至一个镜像中统一打包，方便 搭建开发测试环境 在公司搭建私有镜像仓库。创建公共镜像源，如：tomcat+jdk镜像、zabbix镜像、jdk镜像、vsftp镜像 搭建生产环境 在公司生成产品镜像。使用docker export 导出至生产环境中，生产环境使用docker import 导入。 安装centos7 由于docker已经放在centos7的extra源内，因此可直接使用yum进行安装配置。 安装1yum install docker 修改docker存储目录 修改配置文件/etc/sysconfig/docker12OPTIONS=&apos;--graph=/data/docker --selinux-enabled --log-driver=journald --signature-verification=false&apos;注：--graph=/data/docker即为修改docker默认路径 配置docker镜像加速器 镜像加速器有利于快速下载镜像,修改如下：1234[root@node249 tomcat8_jre8_supervisor]# cat /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;]&#125; 启停服务&amp;开机自启动1234重新加载配置文件：systemctl daemon-reload启动：systemctl start docker.service停止：systemctl stop docker.service开机自启动：systemctl enable docker.service 镜像操作搜索镜像1docker search centos 获取镜像1docker push docker.io/centos 删除镜像1docker rmi docker.io/centos 制作镜像制作tomcat镜像123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 目录结构[root@node249 tomcat8-jre8]# pwd/data/images/tomcat8-jre8[root@node249 tomcat8-jre8]# tree -L 2.├── Dockerfile├── java│ └── jre1.8.0_192└── tomcat ├── bin ├── conf ├── lib ├── LICENSE ├── logs ├── NOTICE ├── RELEASE-NOTES ├── RUNNING.txt ├── temp ├── webapps └── work10 directories, 5 files# Dockerfile内容[root@node249 tomcat8-jre8]# cat Dockerfile #构建tomcat8基础镜像#引用基础镜像FROM docker.io/centosMAINTAINER yanghang &quot;yanghang0717@139.com&quot;#创建文件夹RUN mkdir -p /data/javaRUN mkdir -p /data/tomcat#将容器时间和主机时间设成一致RUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo &apos;Asia/Shanghai&apos; &gt;/etc/timezone#把当前目录下的文件夹添加到镜像中ADD java /data/javaADD tomcat /data/tomcat#添加环境变量ENV CATALINA_HOME /data/tomcatENV JAVA_HOME /data/java/jre1.8.0_192ENV PATH $CATALINA_HOME/bin:$JAVA_HOME/bin:$PATHWORKDIR $CATALINA_HOME#暴露8080端口EXPOSE 8080#启动时运行tomcatCMD [&quot;/usr/local/tomcat/bin/catalina.sh&quot;, &quot;run&quot;]# 制作tomcat镜像docker build -t tomcat8 .docker build -t centos7_zh . 制作jar镜像1&lt;img src=&quot;https://ae01.alicdn.com/kf/HTB1wl18aMZC2uNjSZFn761xZpXaU.png&quot;&gt; 容器操作根据镜像创建容器创建tomcat容器 注:tomcat开发测试环境建议网络使用桥接网络，生产环境由于使用独立服务器，因此使用host网络模式。1234567891011# 开发测试环境docker run -itd --restart=always -p 10001:8080 -v /data/mvtech/blxx_netmanager/tomcat/logs:/data/tomcat/logs -v /data/mvtech/blxx_netmanager/tomcat/webapps/ROOT:/data/tomcat/webapps/ROOT --privileged --name=blxx_netmanager tomcat8# 生产环境docker run -itd --restart=always --network host -v /data/mvtech/blxx_netmanager/tomcat/logs:/data/tomcat/logs -v /data/mvtech/blxx_netmanager/tomcat/webapps/ROOT:/data/tomcat/webapps/ROOT --privileged --name=blxx_netmanager tomcat8docker run -itd --restart=always -p 10080:8080 -v /blxx/tomcat/logs:/data/tomcat/logs -v /blxx/tomcat/webapps/blxx_mobile:/data/tomcat/webapps/blxx_mobile --privileged --name=blxx_mobile centos7_zh_jdk8_tomcat8docker pull 192.168.2.249/library/centos7_zh_jdk8_tomcat8:v1.0docker run -itd --restart=always -p 10080:8080 -v /blxx/tomcat/logs:/data/tomcat/logs -v /blxx/tomcat/webapps/blxx_mobile:/data/tomcat/webapps/blxx_mobile --privileged --name=blxx_mobile 192.168.2.249/library/centos7_zh_jdk8_tomcat8:v1.0 创建vsftp容器 ftp镜像建议使用host网络模式，因为桥接模式会报错。123456docker run -d --restart=always --network host -v /data/ftp_root/:/home/vsftpd -e FTP_USER=mvtechftp -e FTP_PASS=mvtech123 --privileged --name mvtechftp docker.io/fauria/vsftpddocker run -d --restart=always --network host -v /data/ftp_root/:/home/vsftpd --privileged --name vsftpd vsftpddocker pull 192.168.2.249/library/centos7-vsftpd:latest 创建jdk环境1docker run -itd --restart=always --privileged --name=centos centos /bin/bash 创建centos容器12345678910111213141516171819202122232425262728293031docker run -itd --restart=always --privileged --name=zabbix-server docker.io/centos /usr/sbin/initdocker run -itd --restart=always --network host --privileged -v /data/ftp_root/mvtechftp:/data/ftp_root/mvtechftp --privileged --name=vsftpd 192.168.2.249/library/centos7-vsftpd /usr/sbin/initdocker run -itd --restart=always --network host --privileged -v /data/ftp_root/mvtechftp:/data/ftp_root/mvtechftp --name=centos centos7-vsftpd /usr/sbin/init/data/ftp_root/mvtechftpdocker run -itd --restart=always --network host --privileged --name=zabbix2.4.8 centos7.5-zabbix2.4.8-zh_cn /usr/sbin/initdocker run -itd --restart=always --network host --privileged --name=zabbix2.4.8 zabbix2.4.8 /usr/sbin/initdocker run -itd --restart=always --network host --privileged --name=centos7_zh centos7_zh /usr/sbin/initdocker pull 192.168.2.249/library/zabbix2.4.8:v1.0docker pull 192.168.2.249/library/centosdocker run -itd --restart=always --network host --privileged --name=zabbix2.4.8 192.168.2.249/library/zabbix2.4.8:v1.0 /usr/sbin/initdocker run -itd --restart=always --network host --privileged --name=centos_dw_crawler 192.168.2.249/library/centos /usr/sbin/initdocker run -itd --restart=always --network host --privileged --name=centos_zh 192.168.2.249/library/centos7_zh /usr/sbin/init进入容器内部docker exec -it zabbix2.4.8 /bin/bashdocker run -d --restart=always -p 3306:3306 -v /d/work/docker/mysql5.7.24:/data --privileged --name=mysql:5.7.24 192.168.2.249/library/mysql:5.7.24 /usr/sbin/init 查看容器详情docker inspect test 查看容器日志docker logs -f -t –tail 10 test 导出镜像1docker export zabbix-server &gt; zabbix-server.tar 导入镜像12docker import - za &lt; zabbix-server.tardocker import http://192.168.2.253/file/zabbix-server.tar zabbix2.4.8 搭建Harbor企业级docker仓库环境准备安装docker 请参考第2章 安装docker 安装docker-compose 注：操作系统不自带pip 请自行安装1pip install docker-compose 下载harbor123url地址：https://github.com/goharbor/harbor/releases选择所需版本。我所下载的为：harbor-online-installer-v1.6.0.tgztar -zxf /root/harbor-online-installer-v1.6.0.tgz -C /usr/local/ 修改配置文件harbor.cfg1234hostname = 192.168.2.46 harbor_admin_password = mvtech123self_registration = offproject_creation_restriction = adminonly 安装前检查 安装harbor123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140[root@harbor ~]# /usr/local/harbor/install.sh [Step 0]: checking installation environment ...Note: docker version: 1.13.1Note: docker-compose version: 1.22.0[Step 1]: preparing environment ...Clearing the configuration file: ./common/config/adminserver/envClearing the configuration file: ./common/config/ui/envClearing the configuration file: ./common/config/ui/app.confClearing the configuration file: ./common/config/ui/private_key.pemClearing the configuration file: ./common/config/db/envClearing the configuration file: ./common/config/jobservice/envClearing the configuration file: ./common/config/jobservice/config.ymlClearing the configuration file: ./common/config/registry/config.ymlClearing the configuration file: ./common/config/registry/root.crtClearing the configuration file: ./common/config/registryctl/envClearing the configuration file: ./common/config/registryctl/config.ymlClearing the configuration file: ./common/config/nginx/nginx.confClearing the configuration file: ./common/config/log/logrotate.confloaded secret from file: /data/secretkeyGenerated configuration file: ./common/config/nginx/nginx.confGenerated configuration file: ./common/config/adminserver/envGenerated configuration file: ./common/config/ui/envGenerated configuration file: ./common/config/registry/config.ymlGenerated configuration file: ./common/config/db/envGenerated configuration file: ./common/config/jobservice/envGenerated configuration file: ./common/config/jobservice/config.ymlGenerated configuration file: ./common/config/log/logrotate.confGenerated configuration file: ./common/config/registryctl/envGenerated configuration file: ./common/config/ui/app.confGenerated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crtThe configuration files are ready, please use docker-compose to start the service.[Step 2]: checking existing instance of Harbor ...[Step 3]: starting Harbor ...Creating network &quot;harbor_harbor&quot; with the default driverPulling log (goharbor/harbor-log:v1.6.0)...Trying to pull repository docker.io/goharbor/harbor-log ... v1.6.0: Pulling from docker.io/goharbor/harbor-log51be32cd3c9d: Pull completefd9cdcfcef45: Pull complete4167b797c339: Pull completeb22d11b0e478: Pull completed5aad3df7cee: Pull completebd43de1bbd44: Pull complete7494eff3da48: Pull completeDigest: sha256:27f9e24f28393a6052b71c93b1571f2269e1d3c489f4081996a099ac88ff56ffStatus: Downloaded newer image for docker.io/goharbor/harbor-log:v1.6.0Pulling postgresql (goharbor/harbor-db:v1.6.0)...Trying to pull repository docker.io/goharbor/harbor-db ... v1.6.0: Pulling from docker.io/goharbor/harbor-db51be32cd3c9d: Already exists16bdbb239be8: Pull complete1f2308455a1a: Pull complete886c09e06dee: Pull complete4c74f43fb3f6: Pull completea5a85370032d: Pull completec393ddbdd7fb: Pull completeae546b8414b0: Pull completeDigest: sha256:ee65d512c93860bd4872be296de80c079842a64e2a4002360e720222a87ec346Status: Downloaded newer image for docker.io/goharbor/harbor-db:v1.6.0Pulling redis (goharbor/redis-photon:v1.6.0)...Trying to pull repository docker.io/goharbor/redis-photon ... v1.6.0: Pulling from docker.io/goharbor/redis-photon51be32cd3c9d: Already existsc400e93ba418: Pull complete170ae129f67d: Pull completebffa31ec55cd: Pull complete5b72a97a5506: Pull completeDigest: sha256:4095dc26d6331b4d3c25377bc02d95501c51fbba99f31f9761d321bbc17803afStatus: Downloaded newer image for docker.io/goharbor/redis-photon:v1.6.0Pulling adminserver (goharbor/harbor-adminserver:v1.6.0)...Trying to pull repository docker.io/goharbor/harbor-adminserver ... v1.6.0: Pulling from docker.io/goharbor/harbor-adminserver51be32cd3c9d: Already existsa12ecf0fa8fc: Pull complete3757394ad64f: Pull complete26ceec7e26ff: Pull completee8d90789101d: Pull completeDigest: sha256:c3ca012c2d69099ba4e3bbedc58ffe146fd10aa5129d44cc7d735edf6167959eStatus: Downloaded newer image for docker.io/goharbor/harbor-adminserver:v1.6.0Pulling registry (goharbor/registry-photon:v2.6.2-v1.6.0)...Trying to pull repository docker.io/goharbor/registry-photon ... v2.6.2-v1.6.0: Pulling from docker.io/goharbor/registry-photon51be32cd3c9d: Already existseaf5637d77d9: Pull completec68621c7e44d: Pull complete314e16c23f49: Pull completee3f6c59a8a19: Pull completee4f08365b84c: Pull complete29c822b725fa: Pull completeDigest: sha256:070dcc29fb5b34cdcc982394ead57f598160fd61bd8daee4b2a5f39ea37bd7a0Status: Downloaded newer image for docker.io/goharbor/registry-photon:v2.6.2-v1.6.0Pulling ui (goharbor/harbor-ui:v1.6.0)...Trying to pull repository docker.io/goharbor/harbor-ui ... v1.6.0: Pulling from docker.io/goharbor/harbor-ui51be32cd3c9d: Already existsec6a6b245304: Pull completea88d6c453ccb: Pull completed5e2e9e0086a: Pull completef8d7e9d8512c: Pull completea06b1a705b19: Pull completeDigest: sha256:de332db437b8df6ce05203247cbf97ac9f4953672a8c22be8858aee47a0f435fStatus: Downloaded newer image for docker.io/goharbor/harbor-ui:v1.6.0Pulling jobservice (goharbor/harbor-jobservice:v1.6.0)...Trying to pull repository docker.io/goharbor/harbor-jobservice ... v1.6.0: Pulling from docker.io/goharbor/harbor-jobservice51be32cd3c9d: Already existsffcdeda0f50f: Pull completee69daf7ff175: Pull complete840fbfb5576e: Pull completeDigest: sha256:51d2bf14cd9d1bbf082793a0556ff949937655c67569a86424210a1455f60057Status: Downloaded newer image for docker.io/goharbor/harbor-jobservice:v1.6.0Pulling proxy (goharbor/nginx-photon:v1.6.0)...Trying to pull repository docker.io/goharbor/nginx-photon ... v1.6.0: Pulling from docker.io/goharbor/nginx-photon51be32cd3c9d: Already existsedc138fa5ed7: Pull completeDigest: sha256:3270c6fc3bdaaecd16280592e916e2cfcf7c5eb54ffc46d79b507b625e3fb4c6Status: Downloaded newer image for docker.io/goharbor/nginx-photon:v1.6.0Creating harbor-log ... doneCreating redis ... doneCreating harbor-adminserver ... doneCreating registry ... doneCreating harbor-db ... doneCreating harbor-ui ... doneCreating harbor-jobservice ... doneCreating nginx ... done✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at http://192.168.2.46 . For more details, please visit https://github.com/goharbor/harbor . 使用harbor web访问：http://192.168.2.46 客户端若使用harbor仓库则需进行如下配置12345678#修改配置文件[root@localhost ~]# cat /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;], &quot;insecure-registries&quot;: [&quot;192.168.2.46&quot;]&#125;#重启docker服务[root@localhost ~]# systemctl restart docker push镜像至harbor中登陆 push镜像若为从其他地方pull的镜像需要修改tag12docker tag centos:latest 192.168.2.46/library/centos:latestdocker push 192.168.2.46/library/centos:latest 客户端pull镜像客户端从服务器中拉取镜像1命令如下：docker pull 192.168.2.46/library/zabbix2.4.8:v1.0 查看此客户端的镜像 常见错误：问题1：1234#问题描述：使用systemctl start sshd 执行报错Failed to get D-Bus connection: Operation not permitted解决#解决方法：启动容器时需要使用docker run -tdi --privileged centos /usr/sbin/init，这样就不能使用docker attach进入容器操作命令了，需要使用docker exec -it zabbix-server /bin/bash docker run -itd –restart=always -v /etc/sftp.conf:/etc/sftp/users.conf:ro -v /mvtech/sftp_root:/home –privileged -p 3333:22 –name=sftp docker.io/atmoz/sftp 制作图片视频程序镜像docker run -itd –restart=always –network host –privileged –name=pic 211.103.227.138:24980/library/centos7_zh_jdk8 /usr/sbin/init-v /mvtech/picsort /mvtech/picsort cd /mvtech/python/check_text &amp;&amp; python3 check_text.py &amp;cd /mvtech/python/check_text &amp;&amp; python3 check_text.py &amp;cd /mvtech/python/picscanner180228 &amp;&amp; python monitor_image.py &amp;python3 check_text.py &amp;[ docker run -itd –restart=always -v /mvtech/picsort /mvtech/picsort –privileged –name=pic 211.103.227.138:24980/mvtech/pic /usr/sbin/init-v /mvtech/picsort /mvtech/picsort docker run -d -it –name=”eu_wz” –hostname=”EU-WZ” –memory=”40960m” –mac-address=”aa:bb:cc:04:28:01” –memory-swap=”-1” –env=”LANG=en_US.UTF-8” –env=”JAVA_HOME=/mvtech/apps/java” –env=”JRE_HOME=/mvtech/apps/java/jre” –env=”CLASSPATH=/mvtech/apps/java/lib:/mvtech/apps/java/jre/lib” –env=”PATH=/mvtech/apps/java/bin:/mvtech/apps/java/jre/bin:/mvtech/apps/java:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin” –volume=”/etc/localtime:/etc/localtime:ro” –volume=”/mvtech/apps/java:/mvtech/apps/java:ro” –volume=”/mvtech/apps/docker/eu_wz:/mvtech/apps/isms-backend:ro” –volume=”/mvtech/data/vsftp/eu_wz/:/mvtech/data” –volume=”/mvtech/work/docker/eu_wz/:/mvtech/work” –volume=”/mvtech/logs/docker/eu_wz/:/mvtech/logs” –workdir=”/mvtech” isms-backend:1.0 /bin/bash]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
